# AI æœå‹™è‡ªå‹•åŒ–å°è©±å­˜å„²å°æ¯”è¡¨æ ¼ (2025å¹´æœ€æ–°)

## ğŸ¯ æ ¸å¿ƒé‡é»ï¼šæ¸›å°‘å°è©±å­˜å„²é–‹ç™¼å·¥ä½œé‡

æœ¬è¡¨æ ¼å°ˆæ³¨æ–¼æ¯”è¼ƒå„ AI æœå‹™åœ¨ã€Œè‡ªå‹•åŒ–å­˜å„²å°è©±ã€æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹«åŠ©é–‹ç™¼è€…é¸æ“‡æœ€èƒ½æ¸›å°‘å­˜å„²ç³»çµ±é–‹ç™¼å·¥ä½œé‡çš„è§£æ±ºæ–¹æ¡ˆã€‚

## ğŸš€ **2025å¹´é‡å¤§æ›´æ–°**

### OpenAI Responses API - é©å‘½æ€§å°è©±å­˜å„²
OpenAI åœ¨ 2025å¹´æ¨å‡ºçš„ **Responses API** å¾¹åº•æ”¹è®Šäº†å°è©±å­˜å„²æ–¹å¼ï¼š

- **ä¸€è¡Œä»£ç¢¼æ°¸ä¹…å­˜å„²**ï¼š`store=True` å³å¯è‡ªå‹•å­˜å„²æ‰€æœ‰å°è©±
- **é›¶å­˜å„²å·¥ä½œé‡**ï¼šç„¡éœ€é–‹ç™¼ä»»ä½•å­˜å„²ç³»çµ±
- **è‡ªå‹•ç‹€æ…‹ç®¡ç†**ï¼šç„¡éœ€æ‰‹å‹•ç¶­è­· `messages` åˆ—è¡¨
- **éŸ¿æ‡‰æŒä¹…åŒ–**ï¼šè‡ªå‹•å­˜å„²æ‰€æœ‰å°è©±æ­·å²
- **å¯¦æ™‚äº‹ä»¶æµ**ï¼šç›£è½ `response.created`ã€`response.in_progress`ã€`response.failed` ç­‰äº‹ä»¶
- **éŸ¿æ‡‰ç®¡ç†**ï¼šæ”¯æ´æª¢ç´¢ã€åˆªé™¤æ­·å²éŸ¿æ‡‰

é€™ä½¿å¾— OpenAI æˆç‚ºè‡ªå‹•åŒ–å­˜å„²ç¨‹åº¦æœ€é«˜çš„ä¼æ¥­ç´šè§£æ±ºæ–¹æ¡ˆã€‚

## ğŸ“Š è‡ªå‹•åŒ–å­˜å„²å°æ¯”è¡¨æ ¼

| æ’åº | AI æœå‹™ | è‡ªå‹•åŒ–å­˜å„²ç¨‹åº¦ | é–‹ç™¼å·¥ä½œé‡ | æŒä¹…åŒ–ç¨‹åº¦ | å­˜å„²æ–¹å¼ | æª¢ç´¢èƒ½åŠ› | API ç«¯é» |
|------|---------|---------------|-----------|-----------|----------|----------|----------|
| ğŸ† 1 | **OpenAI Responses API** | âœ… å®Œå…¨è‡ªå‹• | **é›¶å·¥ä½œé‡** | æ°¸ä¹… | é›²ç«¯è‡ªå‹•å­˜å„² | å¼·å¤§ | `/v1/responses` |
| ğŸ¥ˆ 2 | **Gemini (Google)** | âœ… å®Œå…¨è‡ªå‹• | **æ¥µä½** | æœƒè©±æœŸé–“ | `start_chat()` è‡ªå‹•ç¶­è­· | ä¸­ç­‰ | `/v1beta/models/*/generateContent` |
| ğŸ¥‰ 3 | **Grok (X.ai)** | âœ… å®Œå…¨è‡ªå‹• | **æ¥µä½** | æœƒè©±æœŸé–“ | `chat.append()` è‡ªå‹•ç®¡ç† | ä¸­ç­‰ | `/v1/chat/completions` |
| 4 | **Kimi-K2 (æœˆä¹‹æš—é¢)** | âš ï¸ åŠè‡ªå‹• | **ä¸­ç­‰** | Session æœŸé–“ | Session æ¨¡å¼å…§è‡ªå‹• | ä¸­ç­‰ | `/v1/chat/completions` |
| 5 | **OpenAI å‚³çµ±** | âŒ æ‰‹å‹• | **é«˜** | ç„¡ | æ‰‹å‹• `messages` åˆ—è¡¨ | åŸºç¤ | `/v1/chat/completions` |
| 6 | **Qwen3-Max-Preview** | âš ï¸ åŠè‡ªå‹• | **ä¸­ç­‰** | æœƒè©±æœŸé–“ | ChatML è‡ªå‹•ç¶­è­· + å·¥å…·èª¿ç”¨ç®¡ç† | å¼·å¤§ | `/compatible-mode/v1/chat/completions` |
| 7 | **Qwen å‚³çµ±æ¨¡å¼** | âŒ æ‰‹å‹• | **é«˜** | ç„¡ | æ‰‹å‹• `messages` åˆ—è¡¨ | åŸºç¤ | `/compatible-mode/v1/chat/completions` |
| 8 | **DeepSeek** | âŒ æ‰‹å‹• | **å¾ˆé«˜** | ç„¡ | æ¯æ¬¡å‚³å®Œæ•´æ­·å² | åŸºç¤ | `/chat/completions` |
| 9 | **Claude (Anthropic)** | âŒ æ‰‹å‹• | **æœ€é«˜** | ç„¡ | ç‰¹æ®Šæ ¼å¼æ‰‹å‹•ç®¡ç† | åŸºç¤ | `/v1/messages` |

### ğŸ” è‡ªå‹•åŒ–ç­‰ç´šèªªæ˜

#### ğŸ† **å®Œå…¨è‡ªå‹•åŒ–å­˜å„²** (é–‹ç™¼å·¥ä½œé‡æ¥µä½)
- **OpenAI Responses API**: `store=True` ä¸€è¡Œæå®šæ°¸ä¹…å­˜å„²
- **Gemini**: `start_chat()` è‡ªå‹•ç¶­è­·æœƒè©±æ­·å²  
- **Grok**: `chat.append()` å®Œå…¨è‡ªå‹•ç®¡ç†

#### âš ï¸ **åŠè‡ªå‹•åŒ–å­˜å„²** (ä¸­ç­‰é–‹ç™¼å·¥ä½œé‡)
- **Kimi-K2**: Session æ¨¡å¼å…§è‡ªå‹•å­˜å„²ï¼Œä½†éœ€ç®¡ç† Session ç”Ÿå‘½é€±æœŸ
- **Qwen3-Max-Preview**: ChatML æ ¼å¼è‡ªå‹•ç¶­è­·å°è©±ç‹€æ…‹ï¼Œæ”¯æŒå·¥å…·èª¿ç”¨å’Œæ€ç¶­æ¨¡å¼ç®¡ç†

#### âŒ **å®Œå…¨æ‰‹å‹•å­˜å„²** (é«˜é–‹ç™¼å·¥ä½œé‡)
- **Claude, DeepSeek, Qwen å‚³çµ±æ¨¡å¼, OpenAI å‚³çµ±**: éœ€è¦è‡ªå»ºå®Œæ•´å­˜å„²ç³»çµ±

---

## è©³ç´°ä½¿ç”¨èªªæ˜

### 1. **OpenAI Responses API** (2025å¹´æ–°åŠŸèƒ½) ğŸ†
```python
from openai import OpenAI
client = OpenAI()

# ğŸš€ é›¶å­˜å„²å·¥ä½œé‡ï¼šä¸€è¡Œä»£ç¢¼æ°¸ä¹…å­˜å„²
response = client.responses.create(
    model="gpt-5",  # æ›´æ–°ç‚º GPT-5 æ¨¡å‹
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    store=True,  # ğŸ¯ é—œéµï¼šè‡ªå‹•æ°¸ä¹…å­˜å„²æ‰€æœ‰å°è©±
    verbosity="medium",  # æ–°åƒæ•¸ï¼šæ§åˆ¶å›æ‡‰è©³ç´°ç¨‹åº¦ (low, medium, high)
    reasoning_effort="minimal"  # æ–°åƒæ•¸ï¼šæ§åˆ¶æ¨ç†æ°´å¹³ (minimal for quick responses)
)

# ç¹¼çºŒå°è©± - å®Œå…¨è‡ªå‹•ç®¡ç†
continue_response = client.responses.create(
    model="gpt-5",
    messages=[{"role": "user", "content": "Tell me more"}],
    store=True,
    parent=response.id,  # è‡ªå‹•é—œè¯å°è©±æ­·å²
    verbosity="high"  # ç¤ºä¾‹ï¼šè¨­å®šç‚ºé«˜è©³ç´°åº¦
)

# æª¢ç´¢æ­·å²å°è©±
history = client.responses.list()
specific_response = client.responses.retrieve(response.id)\n```

**ğŸ¯ GPT-5 æ–°åƒæ•¸èªªæ˜ï¼š**\n- **verbosity**: æ§åˆ¶å›æ‡‰çš„è©³ç´°ç¨‹åº¦ï¼ˆlow: ç°¡çŸ­ã€medium: é»˜èªã€high: å…¨é¢ï¼‰ã€‚\n- **reasoning_effort**: èª¿æ•´æ¨ç†æ·±åº¦ï¼ˆminimal: å¿«é€Ÿå›æ‡‰ï¼Œé©åˆç°¡å–®ä»»å‹™ï¼‰ã€‚\n\né€™äº›åƒæ•¸è®“é–‹ç™¼æ›´éˆæ´»ï¼Œæ¸›å°‘ä¸å¿…è¦çš„è¨ˆç®—ã€‚

**ğŸ¯ æ ¸å¿ƒå„ªå‹¢ï¼š**
- **é›¶å­˜å„²å·¥ä½œé‡**ï¼š`store=True` ä¸€è¡Œæå®š
- **è‡ªå‹•ç‹€æ…‹ç®¡ç†**ï¼šç„¡éœ€æ‰‹å‹•ç¶­è­·ä»»ä½•åˆ—è¡¨
- **æ°¸ä¹…æŒä¹…åŒ–**ï¼šé›²ç«¯è‡ªå‹•å­˜å„²ï¼Œæ°¸ä¸ä¸Ÿå¤±
- **å¼·å¤§æª¢ç´¢**ï¼šæ”¯æ´è¤‡é›œæŸ¥è©¢å’Œéæ¿¾
- **äº‹ä»¶ç›£è½**ï¼šå¯¦æ™‚è¿½è¹¤å°è©±ç‹€æ…‹

---

### 2. **OpenAI å‚³çµ±æ¨¡å¼**
```python
from openai import OpenAI
client = OpenAI()

# åˆå§‹åŒ–å°è©±
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
]

# ç™¼é€è«‹æ±‚
response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages
)

# æ‰‹å‹•æ·»åŠ å›æ‡‰åˆ°å°è©±æ­·å²
messages.append(response.choices[0].message)

# æ–°åŠŸèƒ½ï¼šæª¢ç´¢æ­·å²å°è©±
completion_id = response.id
messages_list = client.chat.completions.messages.list(completion_id=completion_id)
```

**ç‰¹é»ï¼š**
- éœ€è¦æ‰‹å‹•ç¶­è­· `messages` åˆ—è¡¨
- 2025å¹´æ–°å¢ï¼šå¯æª¢ç´¢ç‰¹å®š completion çš„ messages
- æ”¯æ´å¤šç¨®è§’è‰²ï¼šsystem, user, assistant, developer

---

### 3. **Gemini (Google)** ğŸ¥ˆ
```python
import google.generativeai as genai

genai.configure(api_key='YOUR_API_KEY')
model = genai.GenerativeModel('gemini-pro')

# ğŸš€ æ¥µä½å·¥ä½œé‡ï¼šè‡ªå‹•å°è©±ç®¡ç†
chat = model.start_chat()
response1 = chat.send_message("Hello!")
response2 = chat.send_message("Tell me more")

# è‡ªå‹•ç¶­è­·æ­·å² - é›¶æ‰‹å‹•ç®¡ç†
print(chat.history)  # å®Œå…¨è‡ªå‹•åŒ…å«æ‰€æœ‰å°è©±

# æ–¹å¼2ï¼šå¸¶åˆå§‹æ­·å²çš„è‡ªå‹•ç®¡ç†
chat = model.start_chat(history=[
    {'role': 'user', 'parts': ['Hello']},
    {'role': 'model', 'parts': ['Hi there!']}
])
```

**ğŸ¯ æ ¸å¿ƒå„ªå‹¢ï¼š**
- **æ¥µä½å·¥ä½œé‡**ï¼š`start_chat()` è‡ªå‹•ç¶­è­·æ­·å²
- **æœƒè©±æœŸé–“æŒä¹…åŒ–**ï¼šè‡ªå‹•ä¿æŒå°è©±ç‹€æ…‹
- **å¤šåª’é«”æ”¯æ´**ï¼šæ”¯æ´åœ–ç‰‡ã€éŸ³é »ç­‰
- **æ™ºèƒ½ç‹€æ…‹ç®¡ç†**ï¼šå…§å»ºå°è©±é‚è¼¯

---

### 4. **Grok (X.ai)** ğŸ¥‰
```python
from grok import GrokClient

client = GrokClient()

# ğŸš€ æ¥µä½å·¥ä½œé‡ï¼šå®Œå…¨è‡ªå‹•åŒ–å°è©±ç®¡ç†
chat = client.create_chat()
response1 = chat.append("Hello, how are you?")
response2 = chat.append("Tell me about AI")

# è‡ªå‹•ç¶­è­·å°è©±æ­·å² - é›¶æ‰‹å‹•æ“ä½œ
print(chat.history)  # è‡ªå‹•åŒ…å«æ‰€æœ‰å°è©±
```

**ğŸ¯ æ ¸å¿ƒå„ªå‹¢ï¼š**
- **æ¥µä½å·¥ä½œé‡**ï¼š`chat.append()` å®Œå…¨è‡ªå‹•ç®¡ç†
- **å…§å»ºç‹€æ…‹ç®¡ç†**ï¼šé–‹ç™¼è€…ç„¡éœ€ä»»ä½•æ‰‹å‹•æ“ä½œ
- **æœ€ç°¡é–‹ç™¼é«”é©—**ï¼šAPI è¨­è¨ˆæœ€ç›´è§€

---

### 5. **Kimi-K2 (æœˆä¹‹æš—é¢)**
```python
import requests

# âš ï¸ åŠè‡ªå‹•ï¼šSession æ¨¡å¼ - éœ€ç®¡ç† Session ç”Ÿå‘½é€±æœŸ
session_response = requests.post(
    'https://api.moonshot.cn/v1/chat/sessions',
    headers={'Authorization': 'Bearer YOUR_API_KEY'},
    json={'model': 'moonshot-v1-8k'}
)
session_id = session_response.json()['id']

# Session å…§è‡ªå‹•å­˜å„²
response = requests.post(
    f'https://api.moonshot.cn/v1/chat/sessions/{session_id}/messages',
    headers={'Authorization': 'Bearer YOUR_API_KEY'},
    json={
        'messages': [
            {'role': 'user', 'content': 'Hello!'}
        ]
    }
)

# æˆ–å‚³çµ±æ‰‹å‹•æ¨¡å¼
messages = [{'role': 'user', 'content': 'Hello!'}]
response = requests.post(
    'https://api.moonshot.cn/v1/chat/completions',
    headers={'Authorization': 'Bearer YOUR_API_KEY'},
    json={
        'model': 'moonshot-v1-8k',
        'messages': messages
    }
)
```

**ç‰¹é»ï¼š**
- **ä¸­ç­‰å·¥ä½œé‡**ï¼šSession æ¨¡å¼å…§è‡ªå‹•ï¼Œä½†éœ€ç®¡ç† Session
- **é›™æ¨¡å¼é¸æ“‡**ï¼šSession è‡ªå‹• + å‚³çµ±æ‰‹å‹•
- **Session æœŸé–“æŒä¹…åŒ–**ï¼šåœ¨ Session ç”Ÿå‘½é€±æœŸå…§è‡ªå‹•å­˜å„²

---

### 6. **Qwen (é˜¿é‡Œé›²)**

#### ğŸ†• **Qwen3-Max-Preview (2025å¹´æ–°ç‰ˆ)**
```python
from transformers import pipeline

# âš ï¸ ä¸­ç­‰å·¥ä½œé‡ï¼šè‡ªå‹•ç¶­è­·å°è©±ç‹€æ…‹
generator = pipeline(
    "text-generation", 
    "Qwen/Qwen3-8B",
    torch_dtype="auto", 
    device_map="auto"
)

# è‡ªå‹•ç¶­è­· messages æ­·å²
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# è‡ªå‹•æ›´æ–°å°è©±æ­·å²
messages = generator(messages, max_new_tokens=32768)[0]["generated_text"]

# ç¹¼çºŒå°è©± - è‡ªå‹•ä¿æŒä¸Šä¸‹æ–‡
messages.append({"role": "user", "content": "In a single sentence."})
messages = generator(messages, max_new_tokens=32768)[0]["generated_text"]
```

#### **å‚³çµ± Qwen æ¨¡å¼**
```python
from dashscope import Generation

# âŒ é«˜å·¥ä½œé‡ï¼šæ‰‹å‹•æ§‹å»ºå°è©±æ­·å²
messages = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
    {'role': 'user', 'content': 'Hello!'}
]

response = Generation.call(
    model='qwen-turbo',
    messages=messages
)

# éœ€è¦æ‰‹å‹•æ·»åŠ å›æ‡‰
messages.append({
    'role': 'assistant', 
    'content': response.output.choices[0].message.content
})
```

**Qwen3-Max-Preview æ–°ç‰¹é»ï¼š**
- **ä¸­ç­‰å·¥ä½œé‡**ï¼šè‡ªå‹•ç¶­è­·å°è©±ç‹€æ…‹å’Œå·¥å…·èª¿ç”¨æ­·å²
- **ChatML æ¨™æº–åŒ–**ï¼šä½¿ç”¨ `